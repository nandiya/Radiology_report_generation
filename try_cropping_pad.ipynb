{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778042a2-dd25-4e40-89bb-f455e1b7539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "import os\n",
    "import sklearn\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import warnings\n",
    "import skimage.io\n",
    "import selective_search\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b496786e-9b4a-4d4a-8a00-3eb05929e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPEG files found:\n",
      "files/p11/p11000183/s50336039/3761aae0-255c0808-86d2121b-88ae172f-b7625d50.jpg\n",
      "files/p11/p11000011/s51029426/ff213473-b64efa18-863f2bad-76181481-30bc30d7.jpg\n",
      "files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg\n",
      "files/p10/p10000032/s50414267/174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962.jpg\n",
      "files/p10/p10000764/s57375967/b79e55c3-735ce5ac-64412506-cdc9ea79-f1af521f.jpg\n",
      "files/p10/p10000764/s57375967/096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4.jpg\n"
     ]
    }
   ],
   "source": [
    "#### listing all jpeg files\n",
    "def list_jpg_files(directory):\n",
    "    jpg_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".jpeg\"):\n",
    "                jpg_files.append(os.path.join(root, file))\n",
    "    return jpg_files\n",
    "\n",
    "directory_path = \"files\"\n",
    "jpg_files = list_jpg_files(directory_path)\n",
    "print(\"JPEG files found:\")\n",
    "for jpg_file in jpg_files:\n",
    "    print(jpg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a733a250-0d09-41db-aed1-9f30036faf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a455a074-92f0-4cc9-9aad-98c94f769608",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert = pd.read_csv(\"mimic-cxr-2.0.0-chexpert.csv\")\n",
    "chexpert = chexpert.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4859bed-8787-495b-b18c-e2cce2c849c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>56699142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  study_id  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0    10000032  50414267          0.0           0.0            0.0    0.0   \n",
       "1    10000032  53189527          0.0           0.0            0.0    0.0   \n",
       "2    10000032  53911762          0.0           0.0            0.0    0.0   \n",
       "3    10000032  56699142          0.0           0.0            0.0    0.0   \n",
       "4    10000764  57375967          0.0           0.0            1.0    0.0   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         0.0       0.0          0.0           0.0   \n",
       "1                         0.0       0.0          0.0           0.0   \n",
       "2                         0.0       0.0          0.0           0.0   \n",
       "3                         0.0       0.0          0.0           0.0   \n",
       "4                         0.0       0.0          0.0           0.0   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         1.0               0.0            0.0        0.0           0.0   \n",
       "1         1.0               0.0            0.0        0.0           0.0   \n",
       "2         1.0               0.0            0.0        0.0           0.0   \n",
       "3         1.0               0.0            0.0        0.0           0.0   \n",
       "4         0.0               0.0            0.0       -1.0           0.0   \n",
       "\n",
       "   Support Devices  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chexpert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f2182c-c956-4361-bc60-dc731ddf502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert['patient_id'] = chexpert['subject_id'].astype(str) + '_' + chexpert['study_id'].astype(str)\n",
    "chexpert['diseases_label'] = chexpert.drop(['subject_id', 'study_id', 'patient_id'], axis=1).values.tolist()\n",
    "chexpert = chexpert[['patient_id', 'diseases_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae46191a-2452-4671-b176-ea0bd80e44b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chexpert[\"diseases_label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805d368-a1dd-40ad-a355-5aede61dab32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871d28d-bae2-4548-9403-fa6c71df859d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c90fd5-3fad-4d75-abed-905f777efbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all regions\n",
    "directory = 'scene_graph'\n",
    "\n",
    "list\n",
    "length_data = 0\n",
    "list_regions = [\"right lung\", \"right costophrenic angle\",\"right cardiophrenic angle\",\"right hemidiaphragm\",\n",
    "                \"left lung\", \"left costophrenic angle\",\"left hemidiaphragm\",\"trachea\",\"right clavicle\",\n",
    "               \"left clavicle\",\"aortic arch\",\"mediastinum\",\"cavoatrial junction\",\"left cardiophrenic angle\",\n",
    "               \"right atrium\"]\n",
    "dict_image = {}\n",
    "dict_progress = {}\n",
    "no_region_image_id = []\n",
    "length_data_relation = 0\n",
    "for filename in os.scandir(directory):\n",
    "    name = filename.name.split(\"_\")[0]\n",
    "    if filename.path.endswith(\".json\"):\n",
    "        for image_path in jpg_files:\n",
    "            path_name = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "            if path_name == name :\n",
    "                length_data = length_data +1\n",
    "                f = open(filename)\n",
    "                data = json.load(f)\n",
    "                list_region = []\n",
    "                count = len(data[\"objects\"])\n",
    "                if count > 15 :\n",
    "                    for region in data[\"objects\"]:\n",
    "                        if region[\"bbox_name\"] in list_regions:\n",
    "                            list_region.append([region[\"original_x1\"], region[\"original_y1\"], region[\"original_x2\"],region[\"original_y2\"]])\n",
    "                    dict_image[image_path] = list_region\n",
    "                    progression_label = {\"Better\":0, \"Worse\" : 0, \"Stable\" :0}\n",
    "                    if len(data[\"relationships\"]) >= 1 :\n",
    "                        for relation in data[\"relationships\"]:\n",
    "                            if relation[\"predicate\"] == \"Worse\":\n",
    "                                progression_label[\"Worse\"] = 1\n",
    "                            elif relation[\"predicate\"] == \"Better\" :\n",
    "                                progression_label[\"Better\"] = 1\n",
    "                            else :\n",
    "                                progression_label[\"Stable\"] = 1    \n",
    "                    dict_progress[image_path] = list(progression_label.values())\n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8005896d-c4bc-4fed-b405-9cce3f9848cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg\n",
      "[0, 0, 0]\n",
      "------\n",
      "files/p10/p10000764/s57375967/096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4.jpg\n",
      "[0, 0, 0]\n",
      "------\n",
      "files/p11/p11000011/s51029426/ff213473-b64efa18-863f2bad-76181481-30bc30d7.jpg\n",
      "[0, 0, 0]\n",
      "------\n",
      "files/p11/p11000183/s50336039/3761aae0-255c0808-86d2121b-88ae172f-b7625d50.jpg\n",
      "[0, 0, 1]\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for i in dict_progress:\n",
    "    print(i)\n",
    "    print(dict_progress[i])\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52df297-bdf1-4799-8079-4bedf8218459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg\n",
      "15\n",
      "==================\n",
      "files/p10/p10000764/s57375967/096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4.jpg\n",
      "15\n",
      "==================\n",
      "files/p11/p11000011/s51029426/ff213473-b64efa18-863f2bad-76181481-30bc30d7.jpg\n",
      "15\n",
      "==================\n",
      "files/p11/p11000183/s50336039/3761aae0-255c0808-86d2121b-88ae172f-b7625d50.jpg\n",
      "15\n",
      "==================\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in dict_image:\n",
    "    print(i)\n",
    "    print(len(dict_image[i]))\n",
    "    print(\"==================\")\n",
    "print(len(list_regions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e54ae0-baf6-4f44-bb7e-936ba3f978d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage1 prep\n",
    "disease_labels = []\n",
    "for i in dict_image:\n",
    "    img_path = i.split(\"/\")\n",
    "    subject_id = img_path[2][1:]\n",
    "    study_id = img_path[3][1:]\n",
    "    tmp_str = subject_id+\"_\"+study_id\n",
    "    for index, row in chexpert.iterrows():\n",
    "        if tmp_str == row[\"patient_id\"]:\n",
    "            disease_labels.append(row[\"diseases_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eb2086c-b8e6-44c4-8ca5-17ce3df62fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a74c81-47a8-47a8-9ec7-bd906b2251a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cropping all images\n",
    "def region_feature_preprocessing(image, bounding_box):\n",
    "\n",
    "    cropped_image = image.crop(bounding_box)\n",
    "    image_array = np.array(cropped_image)\n",
    "    padded_image = image_array\n",
    "    \n",
    "    height, width = cropped_image.size \n",
    "    \n",
    "    max_pad = max(height, width)\n",
    "    if height < max_pad :\n",
    "        pad_height = max_pad - height\n",
    "        padded_image = np.pad(image_array, ((0, pad_height), (0,0),(0,0)), mode='constant')\n",
    "        \n",
    "    if width < max_pad :\n",
    "        pad_width = max_pad - width\n",
    "        padded_image = np.pad(image_array, ((0, 0), (0,pad_width), (0,0)), mode='constant')\n",
    "    return Image.fromarray(padded_image)\n",
    "\n",
    "\n",
    "#image_inputs = {}\n",
    "#for image in dict_image:\n",
    "#    aa = region_feature_preprocessing(image, dict_image[image])\n",
    "#    image_inputs[image] = aa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6e4a1c-bf5c-419b-bd64-7cef32868325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e5f6a-8904-4f52-b86b-903da467ca46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c768d-62b3-4fc7-ab02-cffb5fe8a989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29647bc0-d3d8-453d-8543-14763b3d0a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7ac39065-fa76-4e79-9beb-90e68a24c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_input_tensors = torch.tensor(image_inputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95686055-b680-435f-8f7d-857f85823d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 1024])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_input_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7b5997-8bba-44aa-9f97-68ea661202fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### trying densenet121 on original image 224x224\n",
    "\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# Remove the original classifier (fully connected layers)\n",
    "model_features = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Define global average pooling layer\n",
    "global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))  # Perform global average pooling to (1, 1)\n",
    "\n",
    "\n",
    "# Combine the layers in a Sequential module\n",
    "model = nn.Sequential(model_features, global_avg_pooling, nn.Flatten())\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d238139-0183-41ee-a085-8edb60cb49bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### extracting the image features from the original image\n",
    "def image_global_feature_extraction(image_path, model):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the input image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    print(type(image))\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    extracted_features = model(image_tensor)\n",
    "    return extracted_features\n",
    "\n",
    "def image_local_feature_extraction(image_path, bounding_boxes, model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "    ])\n",
    "    region_features = []    \n",
    "    # Load and preprocess the input image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    for region in bounding_boxes:\n",
    "        cropped_image = region_feature_preprocessing(image, region)  # Crop the image\n",
    "        image_tensor = transform(cropped_image).unsqueeze(0)  # Apply transformations\n",
    "        extracted_features = model(image_tensor)\n",
    "        region_features.append(extracted_features)\n",
    "    concat_region = torch.cat(region_features, dim=0)    \n",
    "    return concat_region\n",
    "\n",
    "\n",
    "\n",
    "full_image_features = []\n",
    "for image in dict_image:\n",
    "    global_features = image_global_feature_extraction(image,model)\n",
    "    local_features = image_local_feature_extraction(image,dict_image[image], model)\n",
    "    combined_features = torch.cat((global_features, local_features), dim=0)\n",
    "    full_image_features.append(combined_features)\n",
    "\n",
    "input_features = torch.stack(full_image_features, dim=0)\n",
    "input_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a42304cd-4a23-4d8e-8799-bdba862301d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(disease_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "582480bd-51e9-448f-91c2-fc4c3d244d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor size: torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, num_heads, hidden_size, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(d_model=input_size, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # tgt shape: [batch_size, tgt_seq_len, input_size]\n",
    "        # memory shape: [batch_size, src_seq_len, input_size]\n",
    "        transformer_output = self.transformer_decoder(tgt, memory)\n",
    "        output = self.fc(transformer_output[:, -1, :])  # Reduce the sequence length dimension\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "input_size = 1024\n",
    "output_size = 14\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "hidden_size = 2048\n",
    "\n",
    "# Instantiate the transformer decoder model\n",
    "model = TransformerDecoder(input_size, output_size, num_layers, num_heads, hidden_size)\n",
    "\n",
    "# Example input tensors\n",
    "tgt_tensor = torch.randn(4, 16, 1024)  # Target sequence\n",
    "memory_tensor = torch.randn(4, 16, 1024)  # Encoder output (memory)\n",
    "\n",
    "# Forward pass\n",
    "output_tensor = model(tgt_tensor, memory_tensor)\n",
    "\n",
    "print(\"Output tensor size:\", output_tensor.size())  # Expected output size: [4, 14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58d4a49e-fc28-45e0-b020-22c5dc512dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83f9535e-e1f3-493c-94a2-0141c83adfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 14])\n",
      "Epoch [1/1000], Loss: 5.7957\n",
      "torch.Size([4, 14])\n",
      "Epoch [2/1000], Loss: 2.2883\n",
      "torch.Size([4, 14])\n",
      "Epoch [3/1000], Loss: 1.1697\n",
      "torch.Size([4, 14])\n",
      "Epoch [4/1000], Loss: -0.2263\n",
      "torch.Size([4, 14])\n",
      "Epoch [5/1000], Loss: -2.5283\n",
      "torch.Size([4, 14])\n",
      "Epoch [6/1000], Loss: -4.7633\n",
      "torch.Size([4, 14])\n",
      "Epoch [7/1000], Loss: -6.6540\n",
      "torch.Size([4, 14])\n",
      "Epoch [8/1000], Loss: -6.9325\n",
      "torch.Size([4, 14])\n",
      "Epoch [9/1000], Loss: -7.6276\n",
      "torch.Size([4, 14])\n",
      "Epoch [10/1000], Loss: -9.0170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_hidden_layer, dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(d_model, ff_hidden_layer)\n",
    "        self.linear2 = nn.Linear(ff_hidden_layer, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x, target_mask):\n",
    "        attn_output, _ = self.self_attention(x, x, x, attn_mask=target_mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, d_model, num_heads, ff_hidden_layer, dropout):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer_block = DecoderBlock(d_model, num_heads, ff_hidden_layer, dropout)\n",
    "        self.linear = nn.Linear(d_model, 32)\n",
    "        self.linear2 = nn.Linear(d_model, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        tgt_mask = generate_square_subsequent_mask(x.size(0))\n",
    "        x = self.transformer_block(x, tgt_mask)\n",
    "        output = self.linear(x)\n",
    "        output2 = self.linear2(output.view(output.size(0),-1))\n",
    "        output = self.softmax(output2)\n",
    "        return output\n",
    "\n",
    "# Function to generate a square subsequent mask\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a mask to prevent attention to future positions.\"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Model parameters\n",
    "input_size = 1024  # Dimensionality of input\n",
    "output_size = 14   # Dimensionality of output\n",
    "d_model = 512      # Dimensionality of the model\n",
    "num_heads = 8      # Number of attention heads\n",
    "ff_hidden_layer = 2048  # Hidden layer size in feedforward network within transformer block\n",
    "dropout = 0.1      # Dropout rate\n",
    "num_layers = 6     # Number of decoder layers\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerDecoder(input_size, output_size, d_model, num_heads, ff_hidden_layer, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Create a tensor representing batch size, sequence length, and input size\n",
    "input_tensor = torch.randn(4, 16, 1024)\n",
    "for epoch in range(10):  # Training for 100 epochs\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_tensor)  # Forward pass\n",
    "    print(output.shape)\n",
    "    # Calculate loss with attention\n",
    "    loss = criterion(output, target)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 1000, loss.item()))\n",
    "# Forward pass through the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e596266-9950-4c5f-befe-95de5b1137f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b782e26-6380-43ff-a7bc-deea89a66067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "561576db-b510-4763-a522-a922aade7d59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AddNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m look_ahead_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# You need to define this mask\u001b[39;00m\n\u001b[1;32m    152\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# You need to define this mask\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m decoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoder(input_seq, encoder_output, look_ahead_mask, padding_mask))\n",
      "Cell \u001b[0;32mIn[58], line 116\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[0;34m(self, output_size, max_seq_length, num_layers, h, d_k, d_v, d_model, d_ff, rate)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(max_seq_length, d_model)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(rate)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)])\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, output_size)\n",
      "Cell \u001b[0;32mIn[58], line 116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(max_seq_length, d_model)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(rate)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)])\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, output_size)\n",
      "Cell \u001b[0;32mIn[58], line 73\u001b[0m, in \u001b[0;36mDecoderLayer.__init__\u001b[0;34m(self, h, d_model, d_k, d_v, d_ff, rate)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attention1 \u001b[38;5;241m=\u001b[39m MultiHeadAttention(d_model, h, rate)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(rate)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_norm1 \u001b[38;5;241m=\u001b[39m \u001b[43mAddNorm\u001b[49m(d_model)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attention2 \u001b[38;5;241m=\u001b[39m MultiHeadAttention(d_model, h, rate)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AddNorm' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Perform linear transformation and split into multiple heads\n",
    "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # Reshape and concatenate heads\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Apply final linear layer\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, h, d_model, d_k, d_v, d_ff, rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multihead_attention1 = MultiHeadAttention(d_model, h, rate)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.add_norm1 = AddNorm(d_model)\n",
    "        self.multihead_attention2 = MultiHeadAttention(d_model, h, rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        self.add_norm2 = AddNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.dropout3 = nn.Dropout(rate)\n",
    "        self.add_norm3 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, look_ahead_mask, padding_mask):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, look_ahead_mask)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        add_norm_output1 = self.add_norm1(x, multihead_output1)\n",
    "\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(add_norm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        add_norm_output2 = self.add_norm2(add_norm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(add_norm_output2)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(add_norm_output2, feedforward_output)\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, max_seq_length, num_layers, h, d_k, d_v, d_model, d_ff, rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_length, d_model)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, output_target, encoder_output, look_ahead_mask, padding_mask):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output)\n",
    "\n",
    "        # Pass on the positional encoded values to each decoder layer\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, encoder_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "        # Apply a fully connected layer\n",
    "        output = self.fc(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Usage example\n",
    "output_size = 14  # Output size for the decoder\n",
    "max_seq_length = 16  # Maximum length of the output sequence\n",
    "num_layers = 6  # Number of layers in the decoder stack\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 1024  # Dimensionality of the model sub-layers' outputs\n",
    "rate = 0.1  # Dropout rate\n",
    "\n",
    "batch_size = 4  # Batch size from the training process\n",
    "\n",
    "input_seq = torch.randn(batch_size, max_seq_length, d_model)\n",
    "encoder_output = torch.randn(batch_size, max_seq_length, d_model)\n",
    "look_ahead_mask = None  # You need to define this mask\n",
    "padding_mask = None  # You need to define this mask\n",
    "\n",
    "decoder = Decoder(output_size, max_seq_length, num_layers, h, d_k, d_v, d_model, d_ff, rate)\n",
    "print(decoder(input_seq, encoder_output, look_ahead_mask, padding_mask))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
